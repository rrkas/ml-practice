{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17cc5def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 23:34:01.081817: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86662cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 23:34:04.001130: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "X_train = tf.convert_to_tensor(\n",
    "    pd.read_csv(\"./data/x_train.csv\").to_numpy(), dtype=float\n",
    ")\n",
    "y_train = tf.convert_to_tensor(\n",
    "    pd.read_csv(\"./data/y_train.csv\").to_numpy(), dtype=float\n",
    ")\n",
    "\n",
    "X_test = tf.convert_to_tensor(pd.read_csv(\"./data/x_test.csv\").to_numpy(), dtype=float)\n",
    "y_test = tf.convert_to_tensor(pd.read_csv(\"./data/y_test.csv\").to_numpy(), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78c97c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# INITIALIZE WEIGHTS AND BIAS\n",
    "# -----------------------------\n",
    "\n",
    "# Create a weight matrix (W) with random initial values\n",
    "# Shape [7, 1] means there are 7 input features and 1 output\n",
    "# 'tf.Variable' means this value can change (learned during training)\n",
    "W = tf.Variable(tf.random.normal([X_train.shape[1], 1]), name=\"weights\")\n",
    "\n",
    "# Create a bias term (b), starting from zero\n",
    "# Shape [1] means one bias value added to every prediction\n",
    "b = tf.Variable(tf.zeros([1]), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd4ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# SET LEARNING RATE\n",
    "# -----------------------------\n",
    "\n",
    "# Learning rate (lr) controls how big a step we take when updating weights\n",
    "# Smaller = slower learning, larger = faster but may overshoot\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6133cd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/10000000], Loss: 17355841536.0000\n",
      "Epoch [100/10000000], Loss: 14824551424.0000\n",
      "Epoch [150/10000000], Loss: 14054148096.0000\n",
      "Epoch [200/10000000], Loss: 13645386752.0000\n",
      "Epoch [250/10000000], Loss: 13359970304.0000\n",
      "Epoch [300/10000000], Loss: 13136565248.0000\n",
      "Epoch [350/10000000], Loss: 12953476096.0000\n",
      "Epoch [400/10000000], Loss: 12799982592.0000\n",
      "Epoch [450/10000000], Loss: 12669357056.0000\n",
      "Epoch [500/10000000], Loss: 12556845056.0000\n",
      "Epoch [550/10000000], Loss: 12458892288.0000\n",
      "Epoch [600/10000000], Loss: 12372782080.0000\n",
      "Epoch [650/10000000], Loss: 12296401920.0000\n",
      "Epoch [700/10000000], Loss: 12228084736.0000\n",
      "Epoch [750/10000000], Loss: 12166522880.0000\n",
      "Epoch [800/10000000], Loss: 12110661632.0000\n",
      "Epoch [850/10000000], Loss: 12059657216.0000\n",
      "Epoch [900/10000000], Loss: 12012832768.0000\n",
      "Epoch [950/10000000], Loss: 11969631232.0000\n",
      "Epoch [1000/10000000], Loss: 11929598976.0000\n",
      "Epoch [1050/10000000], Loss: 11892366336.0000\n",
      "Epoch [1100/10000000], Loss: 11857622016.0000\n",
      "Epoch [1150/10000000], Loss: 11825106944.0000\n",
      "Epoch [1200/10000000], Loss: 11794603008.0000\n",
      "Epoch [1250/10000000], Loss: 11765927936.0000\n",
      "Epoch [1300/10000000], Loss: 11738918912.0000\n",
      "Epoch [1350/10000000], Loss: 11713444864.0000\n",
      "Epoch [1400/10000000], Loss: 11689380864.0000\n",
      "Epoch [1450/10000000], Loss: 11666624512.0000\n",
      "Epoch [1500/10000000], Loss: 11645083648.0000\n",
      "Epoch [1550/10000000], Loss: 11624675328.0000\n",
      "Epoch [1600/10000000], Loss: 11605326848.0000\n",
      "Epoch [1650/10000000], Loss: 11586968576.0000\n",
      "Epoch [1700/10000000], Loss: 11569541120.0000\n",
      "Epoch [1750/10000000], Loss: 11552988160.0000\n",
      "Epoch [1800/10000000], Loss: 11537258496.0000\n",
      "Epoch [1850/10000000], Loss: 11522306048.0000\n",
      "Epoch [1900/10000000], Loss: 11508084736.0000\n",
      "Epoch [1950/10000000], Loss: 11494555648.0000\n",
      "Epoch [2000/10000000], Loss: 11481680896.0000\n",
      "Epoch [2050/10000000], Loss: 11469423616.0000\n",
      "Epoch [2100/10000000], Loss: 11457751040.0000\n",
      "Epoch [2150/10000000], Loss: 11446632448.0000\n",
      "Epoch [2200/10000000], Loss: 11436039168.0000\n",
      "Epoch [2250/10000000], Loss: 11425941504.0000\n",
      "Epoch [2300/10000000], Loss: 11416316928.0000\n",
      "Epoch [2350/10000000], Loss: 11407136768.0000\n",
      "Epoch [2400/10000000], Loss: 11398381568.0000\n",
      "Epoch [2450/10000000], Loss: 11390026752.0000\n",
      "Epoch [2500/10000000], Loss: 11382054912.0000\n",
      "Epoch [2550/10000000], Loss: 11374442496.0000\n",
      "Epoch [2600/10000000], Loss: 11367175168.0000\n",
      "Epoch [2650/10000000], Loss: 11360232448.0000\n",
      "Epoch [2700/10000000], Loss: 11353597952.0000\n",
      "Epoch [2750/10000000], Loss: 11347259392.0000\n",
      "Epoch [2800/10000000], Loss: 11341199360.0000\n",
      "Epoch [2850/10000000], Loss: 11335402496.0000\n",
      "Epoch [2900/10000000], Loss: 11329858560.0000\n",
      "Epoch [2950/10000000], Loss: 11324553216.0000\n",
      "Epoch [3000/10000000], Loss: 11319476224.0000\n",
      "Epoch [3050/10000000], Loss: 11314615296.0000\n",
      "Epoch [3100/10000000], Loss: 11309959168.0000\n",
      "Epoch [3150/10000000], Loss: 11305497600.0000\n",
      "Epoch [3200/10000000], Loss: 11301222400.0000\n",
      "Epoch [3250/10000000], Loss: 11297123328.0000\n",
      "Epoch [3300/10000000], Loss: 11293192192.0000\n",
      "Epoch [3350/10000000], Loss: 11289420800.0000\n",
      "Epoch [3400/10000000], Loss: 11285803008.0000\n",
      "Epoch [3450/10000000], Loss: 11282327552.0000\n",
      "Epoch [3500/10000000], Loss: 11278989312.0000\n",
      "Epoch [3550/10000000], Loss: 11275784192.0000\n",
      "Epoch [3600/10000000], Loss: 11272702976.0000\n",
      "Epoch [3650/10000000], Loss: 11269741568.0000\n",
      "Epoch [3700/10000000], Loss: 11266891776.0000\n",
      "Epoch [3750/10000000], Loss: 11264151552.0000\n",
      "Epoch [3800/10000000], Loss: 11261513728.0000\n",
      "Epoch [3850/10000000], Loss: 11258972160.0000\n",
      "Epoch [3900/10000000], Loss: 11256525824.0000\n",
      "Epoch [3950/10000000], Loss: 11254168576.0000\n",
      "Epoch [4000/10000000], Loss: 11251895296.0000\n",
      "Epoch [4050/10000000], Loss: 11249702912.0000\n",
      "Epoch [4100/10000000], Loss: 11247589376.0000\n",
      "Epoch [4150/10000000], Loss: 11245547520.0000\n",
      "Epoch [4200/10000000], Loss: 11243577344.0000\n",
      "Epoch [4250/10000000], Loss: 11241675776.0000\n",
      "Epoch [4300/10000000], Loss: 11239835648.0000\n",
      "Epoch [4350/10000000], Loss: 11238057984.0000\n",
      "Epoch [4400/10000000], Loss: 11236338688.0000\n",
      "Epoch [4450/10000000], Loss: 11234677760.0000\n",
      "Epoch [4500/10000000], Loss: 11233068032.0000\n",
      "Epoch [4550/10000000], Loss: 11231509504.0000\n",
      "Epoch [4600/10000000], Loss: 11230001152.0000\n",
      "Epoch [4650/10000000], Loss: 11228539904.0000\n",
      "Epoch [4700/10000000], Loss: 11227122688.0000\n",
      "Epoch [4750/10000000], Loss: 11225749504.0000\n",
      "Epoch [4800/10000000], Loss: 11224416256.0000\n",
      "Epoch [4850/10000000], Loss: 11223123968.0000\n",
      "Epoch [4900/10000000], Loss: 11221867520.0000\n",
      "Epoch [4950/10000000], Loss: 11220647936.0000\n",
      "Epoch [5000/10000000], Loss: 11219464192.0000\n",
      "Epoch [5050/10000000], Loss: 11218313216.0000\n",
      "Epoch [5100/10000000], Loss: 11217195008.0000\n",
      "Epoch [5150/10000000], Loss: 11216106496.0000\n",
      "Epoch [5200/10000000], Loss: 11215046656.0000\n",
      "Epoch [5250/10000000], Loss: 11214016512.0000\n",
      "Epoch [5300/10000000], Loss: 11213012992.0000\n",
      "Epoch [5350/10000000], Loss: 11212036096.0000\n",
      "Epoch [5400/10000000], Loss: 11211083776.0000\n",
      "Epoch [5450/10000000], Loss: 11210157056.0000\n",
      "Epoch [5500/10000000], Loss: 11209251840.0000\n",
      "Epoch [5550/10000000], Loss: 11208370176.0000\n",
      "Epoch [5600/10000000], Loss: 11207510016.0000\n",
      "Epoch [5650/10000000], Loss: 11206670336.0000\n",
      "Epoch [5700/10000000], Loss: 11205850112.0000\n",
      "Epoch [5750/10000000], Loss: 11205049344.0000\n",
      "Epoch [5800/10000000], Loss: 11204269056.0000\n",
      "Epoch [5850/10000000], Loss: 11203503104.0000\n",
      "Epoch [5900/10000000], Loss: 11202756608.0000\n",
      "Epoch [5950/10000000], Loss: 11202026496.0000\n",
      "Epoch [6000/10000000], Loss: 11201313792.0000\n",
      "Epoch [6050/10000000], Loss: 11200614400.0000\n",
      "Epoch [6100/10000000], Loss: 11199932416.0000\n",
      "Epoch [6150/10000000], Loss: 11199263744.0000\n",
      "Epoch [6200/10000000], Loss: 11198609408.0000\n",
      "Epoch [6250/10000000], Loss: 11197967360.0000\n",
      "Epoch [6300/10000000], Loss: 11197340672.0000\n",
      "Epoch [6350/10000000], Loss: 11196725248.0000\n",
      "Epoch [6400/10000000], Loss: 11196122112.0000\n",
      "Epoch [6450/10000000], Loss: 11195531264.0000\n",
      "Epoch [6500/10000000], Loss: 11194952704.0000\n",
      "Epoch [6550/10000000], Loss: 11194384384.0000\n",
      "Epoch [6600/10000000], Loss: 11193826304.0000\n",
      "Epoch [6650/10000000], Loss: 11193281536.0000\n",
      "Epoch [6700/10000000], Loss: 11192744960.0000\n",
      "Epoch [6750/10000000], Loss: 11192218624.0000\n",
      "Epoch [6800/10000000], Loss: 11191700480.0000\n",
      "Epoch [6850/10000000], Loss: 11191193600.0000\n",
      "Epoch [6900/10000000], Loss: 11190694912.0000\n",
      "Epoch [6950/10000000], Loss: 11190206464.0000\n",
      "Epoch [7000/10000000], Loss: 11189725184.0000\n",
      "Epoch [7050/10000000], Loss: 11189254144.0000\n",
      "Epoch [7100/10000000], Loss: 11188788224.0000\n",
      "Epoch [7150/10000000], Loss: 11188333568.0000\n",
      "Epoch [7200/10000000], Loss: 11187884032.0000\n",
      "Epoch [7250/10000000], Loss: 11187442688.0000\n",
      "Epoch [7300/10000000], Loss: 11187008512.0000\n",
      "Epoch [7350/10000000], Loss: 11186581504.0000\n",
      "Epoch [7400/10000000], Loss: 11186161664.0000\n",
      "Epoch [7450/10000000], Loss: 11185748992.0000\n",
      "Epoch [7500/10000000], Loss: 11185343488.0000\n",
      "Epoch [7550/10000000], Loss: 11184942080.0000\n",
      "Epoch [7600/10000000], Loss: 11184547840.0000\n",
      "Epoch [7650/10000000], Loss: 11184160768.0000\n",
      "Epoch [7700/10000000], Loss: 11183777792.0000\n",
      "Epoch [7750/10000000], Loss: 11183401984.0000\n",
      "Epoch [7800/10000000], Loss: 11183032320.0000\n",
      "Epoch [7850/10000000], Loss: 11182667776.0000\n",
      "Epoch [7900/10000000], Loss: 11182307328.0000\n",
      "Epoch [7950/10000000], Loss: 11181955072.0000\n",
      "Epoch [8000/10000000], Loss: 11181605888.0000\n",
      "Epoch [8050/10000000], Loss: 11181260800.0000\n",
      "Epoch [8100/10000000], Loss: 11180921856.0000\n",
      "Epoch [8150/10000000], Loss: 11180588032.0000\n",
      "Epoch [8200/10000000], Loss: 11180260352.0000\n",
      "Epoch [8250/10000000], Loss: 11179933696.0000\n",
      "Epoch [8300/10000000], Loss: 11179613184.0000\n",
      "Epoch [8350/10000000], Loss: 11179298816.0000\n",
      "Epoch [8400/10000000], Loss: 11178986496.0000\n",
      "Epoch [8450/10000000], Loss: 11178679296.0000\n",
      "Epoch [8500/10000000], Loss: 11178376192.0000\n",
      "Epoch [8550/10000000], Loss: 11178077184.0000\n",
      "Epoch [8600/10000000], Loss: 11177783296.0000\n",
      "Epoch [8650/10000000], Loss: 11177491456.0000\n",
      "Epoch [8700/10000000], Loss: 11177204736.0000\n",
      "Epoch [8750/10000000], Loss: 11176921088.0000\n",
      "Epoch [8800/10000000], Loss: 11176642560.0000\n",
      "Epoch [8850/10000000], Loss: 11176365056.0000\n",
      "Epoch [8900/10000000], Loss: 11176091648.0000\n",
      "Epoch [8950/10000000], Loss: 11175823360.0000\n",
      "Epoch [9000/10000000], Loss: 11175558144.0000\n",
      "Epoch [9050/10000000], Loss: 11175294976.0000\n",
      "Epoch [9100/10000000], Loss: 11175035904.0000\n",
      "Epoch [9150/10000000], Loss: 11174779904.0000\n",
      "Epoch [9200/10000000], Loss: 11174528000.0000\n",
      "Epoch [9250/10000000], Loss: 11174278144.0000\n",
      "Epoch [9300/10000000], Loss: 11174032384.0000\n",
      "Epoch [9350/10000000], Loss: 11173787648.0000\n",
      "Epoch [9400/10000000], Loss: 11173548032.0000\n",
      "Epoch [9450/10000000], Loss: 11173310464.0000\n",
      "Epoch [9500/10000000], Loss: 11173074944.0000\n",
      "Epoch [9550/10000000], Loss: 11172842496.0000\n",
      "Epoch [9600/10000000], Loss: 11172613120.0000\n",
      "Epoch [9650/10000000], Loss: 11172386816.0000\n",
      "Epoch [9700/10000000], Loss: 11172161536.0000\n",
      "Epoch [9750/10000000], Loss: 11171941376.0000\n",
      "Epoch [9800/10000000], Loss: 11171722240.0000\n",
      "Epoch [9850/10000000], Loss: 11171506176.0000\n",
      "Epoch [9900/10000000], Loss: 11171291136.0000\n",
      "Epoch [9950/10000000], Loss: 11171080192.0000\n",
      "Epoch [10000/10000000], Loss: 11170871296.0000\n",
      "Epoch [10050/10000000], Loss: 11170663424.0000\n",
      "Epoch [10100/10000000], Loss: 11170459648.0000\n",
      "Epoch [10150/10000000], Loss: 11170257920.0000\n",
      "Epoch [10200/10000000], Loss: 11170058240.0000\n",
      "Epoch [10250/10000000], Loss: 11169859584.0000\n",
      "Epoch [10300/10000000], Loss: 11169664000.0000\n",
      "Epoch [10350/10000000], Loss: 11169469440.0000\n",
      "Epoch [10400/10000000], Loss: 11169280000.0000\n",
      "Epoch [10450/10000000], Loss: 11169089536.0000\n",
      "Epoch [10500/10000000], Loss: 11168903168.0000\n",
      "Epoch [10550/10000000], Loss: 11168717824.0000\n",
      "Epoch [10600/10000000], Loss: 11168533504.0000\n",
      "Epoch [10650/10000000], Loss: 11168352256.0000\n",
      "Epoch [10700/10000000], Loss: 11168173056.0000\n",
      "Epoch [10750/10000000], Loss: 11167994880.0000\n",
      "Epoch [10800/10000000], Loss: 11167818752.0000\n",
      "Epoch [10850/10000000], Loss: 11167644672.0000\n",
      "Epoch [10900/10000000], Loss: 11167471616.0000\n",
      "Epoch [10950/10000000], Loss: 11167302656.0000\n",
      "Epoch [11000/10000000], Loss: 11167133696.0000\n",
      "Epoch [11050/10000000], Loss: 11166965760.0000\n",
      "Epoch [11100/10000000], Loss: 11166801920.0000\n",
      "Epoch [11150/10000000], Loss: 11166637056.0000\n",
      "Epoch [11200/10000000], Loss: 11166475264.0000\n",
      "Epoch [11250/10000000], Loss: 11166314496.0000\n",
      "Epoch [11300/10000000], Loss: 11166155776.0000\n",
      "Epoch [11350/10000000], Loss: 11165999104.0000\n",
      "Epoch [11400/10000000], Loss: 11165843456.0000\n",
      "Epoch [11450/10000000], Loss: 11165688832.0000\n",
      "Epoch [11500/10000000], Loss: 11165536256.0000\n",
      "Epoch [11550/10000000], Loss: 11165383680.0000\n",
      "Epoch [11600/10000000], Loss: 11165234176.0000\n",
      "Epoch [11650/10000000], Loss: 11165085696.0000\n",
      "Epoch [11700/10000000], Loss: 11164938240.0000\n",
      "Epoch [11750/10000000], Loss: 11164792832.0000\n",
      "Epoch [11800/10000000], Loss: 11164649472.0000\n",
      "Epoch [11850/10000000], Loss: 11164505088.0000\n",
      "Epoch [11900/10000000], Loss: 11164364800.0000\n",
      "Epoch [11950/10000000], Loss: 11164223488.0000\n",
      "Epoch [12000/10000000], Loss: 11164085248.0000\n",
      "Epoch [12050/10000000], Loss: 11163947008.0000\n",
      "Epoch [12100/10000000], Loss: 11163811840.0000\n",
      "Epoch [12150/10000000], Loss: 11163675648.0000\n",
      "Epoch [12200/10000000], Loss: 11163541504.0000\n",
      "Epoch [12250/10000000], Loss: 11163410432.0000\n",
      "Epoch [12300/10000000], Loss: 11163277312.0000\n",
      "Epoch [12350/10000000], Loss: 11163147264.0000\n",
      "Epoch [12400/10000000], Loss: 11163018240.0000\n",
      "Epoch [12450/10000000], Loss: 11162890240.0000\n",
      "Epoch [12500/10000000], Loss: 11162763264.0000\n",
      "Epoch [12550/10000000], Loss: 11162637312.0000\n",
      "Epoch [12600/10000000], Loss: 11162511360.0000\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# TRAINING LOOP\n",
    "# -----------------------------\n",
    "\n",
    "# Set a small tolerance value — if the loss stops changing by more than this, we’ll stop training early\n",
    "tol = 1e-5\n",
    "\n",
    "# Set the maximum number of epochs (full passes through the dataset)\n",
    "# 1e7 = 10 million (a very large number, used here as an upper limit)\n",
    "epochs = int(1e7)\n",
    "\n",
    "# Store the previous loss value; start with infinity so any real loss will be smaller\n",
    "prev_loss = float(\"inf\")\n",
    "\n",
    "# Repeat the training process for each epoch\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Record operations for automatic differentiation (for computing gradients)\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # ---- Forward pass ----\n",
    "        # Compute predicted outputs using the current weights and bias\n",
    "        # tf.matmul() does matrix multiplication between X (inputs) and W (weights)\n",
    "        # Then add the bias term 'b'\n",
    "        y_pred = tf.matmul(X_train, W) + b\n",
    "\n",
    "        # ---- Compute loss ----\n",
    "        # Mean Squared Error (MSE): average of squared differences\n",
    "        # between actual (y) and predicted (y_pred) values\n",
    "        loss = tf.reduce_mean(tf.square(y_train - y_pred))\n",
    "\n",
    "    # ---- Backward pass ----\n",
    "    # Compute gradients of the loss with respect to weights and bias\n",
    "    grads = tape.gradient(loss, [W, b])\n",
    "\n",
    "    # ---- Update parameters ----\n",
    "    # Manually adjust weights and bias in the opposite direction of the gradient\n",
    "    # W = W - lr * gradient_of_W\n",
    "    W.assign_sub(lr * grads[0])\n",
    "    b.assign_sub(lr * grads[1])\n",
    "\n",
    "    curr_loss = loss.numpy()\n",
    "\n",
    "    # ---- Print progress ----\n",
    "    # Show loss every 50 epochs so you can track improvement\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {curr_loss:.4f}\")\n",
    "\n",
    "    # ---- Early stopping condition ----\n",
    "    # If the loss hasn’t changed much compared to the last epoch, stop training\n",
    "    if abs(curr_loss - prev_loss) <= tol:\n",
    "        break\n",
    "\n",
    "    # Save current loss as previous loss for the next iteration\n",
    "    prev_loss = curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc796074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model/model.tf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(weights=W, bias=b)\n",
    "ckpt.write(\"./model/model.tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35879ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x73ff09cb88f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(weights=W, bias=b)\n",
    "ckpt.read(\"./model/model.tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc30783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 12283815936.0000\n",
      "Root Mean Squared Error (RMSE): 110832.3777\n",
      "Mean Absolute Error (MAE): 87416.5625\n",
      "R2 Score: 0.5036\n"
     ]
    }
   ],
   "source": [
    "# Example inference\n",
    "y_pred = tf.matmul(X_test, W) + b\n",
    "\n",
    "y_test_flat, y_pred_flat = y_test.numpy().flatten(), y_pred.numpy().flatten()\n",
    "\n",
    "mse = mean_squared_error(y_test_flat, y_pred_flat)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_flat, y_pred_flat)\n",
    "r2 = r2_score(y_test_flat, y_pred_flat)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d78bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
