{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17cc5def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 23:43:39.716930: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86662cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 23:43:42.580365: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "X_train = tf.convert_to_tensor(\n",
    "    pd.read_csv(\"./data/x_train.csv\").to_numpy(), dtype=float\n",
    ")\n",
    "y_train = tf.convert_to_tensor(\n",
    "    pd.read_csv(\"./data/y_train.csv\").to_numpy(), dtype=float\n",
    ")\n",
    "\n",
    "X_test = tf.convert_to_tensor(pd.read_csv(\"./data/x_test.csv\").to_numpy(), dtype=float)\n",
    "y_test = tf.convert_to_tensor(pd.read_csv(\"./data/y_test.csv\").to_numpy(), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78c97c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# INITIALIZE WEIGHTS AND BIAS\n",
    "# -----------------------------\n",
    "\n",
    "# Create a weight matrix (W) with random initial values\n",
    "# Shape [7, 1] means there are 7 input features and 1 output\n",
    "# 'tf.Variable' means this value can change (learned during training)\n",
    "W = tf.Variable(tf.random.normal([X_train.shape[1], 1]), name=\"weights\")\n",
    "\n",
    "# Create a bias term (b), starting from zero\n",
    "# Shape [1] means one bias value added to every prediction\n",
    "b = tf.Variable(tf.zeros([1]), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd4ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# SET LEARNING RATE\n",
    "# -----------------------------\n",
    "\n",
    "# Learning rate (lr) controls how big a step we take when updating weights\n",
    "# Smaller = slower learning, larger = faster but may overshoot\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6133cd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/10000000], Loss: 374.3491\n",
      "Epoch [100/10000000], Loss: 93.9323\n",
      "Epoch [150/10000000], Loss: 51.9700\n",
      "Epoch [200/10000000], Loss: 35.5154\n",
      "Epoch [250/10000000], Loss: 25.3794\n",
      "Epoch [300/10000000], Loss: 18.5831\n",
      "Epoch [350/10000000], Loss: 13.9679\n",
      "Epoch [400/10000000], Loss: 10.8277\n",
      "Epoch [450/10000000], Loss: 8.6903\n",
      "Epoch [500/10000000], Loss: 7.2354\n",
      "Epoch [550/10000000], Loss: 6.2450\n",
      "Epoch [600/10000000], Loss: 5.5709\n",
      "Epoch [650/10000000], Loss: 5.1120\n",
      "Epoch [700/10000000], Loss: 4.7996\n",
      "Epoch [750/10000000], Loss: 4.5870\n",
      "Epoch [800/10000000], Loss: 4.4423\n",
      "Epoch [850/10000000], Loss: 4.3438\n",
      "Epoch [900/10000000], Loss: 4.2767\n",
      "Epoch [950/10000000], Loss: 4.2311\n",
      "Epoch [1000/10000000], Loss: 4.2000\n",
      "Epoch [1050/10000000], Loss: 4.1788\n",
      "Epoch [1100/10000000], Loss: 4.1644\n",
      "Epoch [1150/10000000], Loss: 4.1546\n",
      "Epoch [1200/10000000], Loss: 4.1480\n",
      "Epoch [1250/10000000], Loss: 4.1434\n",
      "Epoch [1300/10000000], Loss: 4.1403\n",
      "Epoch [1350/10000000], Loss: 4.1382\n",
      "Epoch [1400/10000000], Loss: 4.1368\n",
      "Epoch [1450/10000000], Loss: 4.1358\n",
      "Epoch [1500/10000000], Loss: 4.1352\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# TRAINING LOOP\n",
    "# -----------------------------\n",
    "\n",
    "# Set a small tolerance value — if the loss stops changing by more than this, we’ll stop training early\n",
    "tol = 1e-5\n",
    "\n",
    "# Set the maximum number of epochs (full passes through the dataset)\n",
    "# 1e7 = 10 million (a very large number, used here as an upper limit)\n",
    "epochs = int(1e7)\n",
    "\n",
    "# Store the previous loss value; start with infinity so any real loss will be smaller\n",
    "prev_loss = float(\"inf\")\n",
    "\n",
    "# Repeat the training process for each epoch\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Record operations for automatic differentiation (for computing gradients)\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # ---- Forward pass ----\n",
    "        # Compute predicted outputs using the current weights and bias\n",
    "        # tf.matmul() does matrix multiplication between X (inputs) and W (weights)\n",
    "        # Then add the bias term 'b'\n",
    "        y_pred = tf.matmul(X_train, W) + b\n",
    "\n",
    "        # ---- Compute loss ----\n",
    "        # Mean Squared Error (MSE): average of squared differences\n",
    "        # between actual (y) and predicted (y_pred) values\n",
    "        loss = tf.reduce_mean(tf.square(y_train - y_pred))\n",
    "\n",
    "    # ---- Backward pass ----\n",
    "    # Compute gradients of the loss with respect to weights and bias\n",
    "    grads = tape.gradient(loss, [W, b])\n",
    "\n",
    "    # ---- Update parameters ----\n",
    "    # Manually adjust weights and bias in the opposite direction of the gradient\n",
    "    # W = W - lr * gradient_of_W\n",
    "    W.assign_sub(lr * grads[0])\n",
    "    b.assign_sub(lr * grads[1])\n",
    "\n",
    "    curr_loss = loss.numpy()\n",
    "\n",
    "    # ---- Print progress ----\n",
    "    # Show loss every 50 epochs so you can track improvement\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {curr_loss:.4f}\")\n",
    "\n",
    "    # ---- Early stopping condition ----\n",
    "    # If the loss hasn’t changed much compared to the last epoch, stop training\n",
    "    if abs(curr_loss - prev_loss) <= tol:\n",
    "        break\n",
    "\n",
    "    # Save current loss as previous loss for the next iteration\n",
    "    prev_loss = curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc796074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model/model.tf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(weights=W, bias=b)\n",
    "ckpt.write(\"./model/model.tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35879ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fed8a7cc200>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(weights=W, bias=b)\n",
    "ckpt.read(\"./model/model.tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc30783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 4.2331\n",
      "Root Mean Squared Error (RMSE): 2.0574\n",
      "Mean Absolute Error (MAE): 1.6442\n",
      "R2 Score: 0.9889\n"
     ]
    }
   ],
   "source": [
    "# Example inference\n",
    "y_pred = tf.matmul(X_test, W) + b\n",
    "\n",
    "y_test_flat, y_pred_flat = y_test.numpy().flatten(), y_pred.numpy().flatten()\n",
    "\n",
    "mse = mean_squared_error(y_test_flat, y_pred_flat)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_flat, y_pred_flat)\n",
    "r2 = r2_score(y_test_flat, y_pred_flat)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d78bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
