{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b83f7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff818718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91dbf92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(pd.read_csv(\"./data/x_train.csv\").to_numpy(), dtype=float)\n",
    "y_train = torch.tensor(pd.read_csv(\"./data/y_train.csv\").to_numpy(), dtype=float)\n",
    "\n",
    "X_test = torch.tensor(pd.read_csv(\"./data/x_test.csv\").to_numpy(), dtype=float)\n",
    "y_test = torch.tensor(pd.read_csv(\"./data/y_test.csv\").to_numpy(), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a218dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# INITIALIZE MODEL, LOSS, AND OPTIMIZER\n",
    "# -----------------------------\n",
    "\n",
    "# Create an instance of your model (a simple linear regression model here)\n",
    "# This model will learn a relationship like: y = w*x + b\n",
    "model = nn.Linear(in_features=X_train.shape[1], out_features=1, dtype=float)\n",
    "\n",
    "# Define the loss function (how we measure the model’s error)\n",
    "# nn.MSELoss() = Mean Squared Error → average of squared differences between predicted and actual values\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer — this updates model weights to reduce the loss\n",
    "# SGD = Stochastic Gradient Descent, a common optimization algorithm\n",
    "# model.parameters() = tells optimizer which parameters to update (weights & biases)\n",
    "# lr=0.01 = learning rate, controls how big a step we take each update\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be9f482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/10000000], Loss: 1036.9485\n",
      "Epoch [100/10000000], Loss: 667.2427\n",
      "Epoch [150/10000000], Loss: 597.7488\n",
      "Epoch [200/10000000], Loss: 554.6269\n",
      "Epoch [250/10000000], Loss: 522.8992\n",
      "Epoch [300/10000000], Loss: 499.1921\n",
      "Epoch [350/10000000], Loss: 481.3932\n",
      "Epoch [400/10000000], Loss: 467.9729\n",
      "Epoch [450/10000000], Loss: 457.8061\n",
      "Epoch [500/10000000], Loss: 450.0629\n",
      "Epoch [550/10000000], Loss: 444.1300\n",
      "Epoch [600/10000000], Loss: 439.5533\n",
      "Epoch [650/10000000], Loss: 435.9964\n",
      "Epoch [700/10000000], Loss: 433.2095\n",
      "Epoch [750/10000000], Loss: 431.0067\n",
      "Epoch [800/10000000], Loss: 429.2494\n",
      "Epoch [850/10000000], Loss: 427.8338\n",
      "Epoch [900/10000000], Loss: 426.6822\n",
      "Epoch [950/10000000], Loss: 425.7360\n",
      "Epoch [1000/10000000], Loss: 424.9510\n",
      "Epoch [1050/10000000], Loss: 424.2934\n",
      "Epoch [1100/10000000], Loss: 423.7377\n",
      "Epoch [1150/10000000], Loss: 423.2640\n",
      "Epoch [1200/10000000], Loss: 422.8572\n",
      "Epoch [1250/10000000], Loss: 422.5053\n",
      "Epoch [1300/10000000], Loss: 422.1990\n",
      "Epoch [1350/10000000], Loss: 421.9310\n",
      "Epoch [1400/10000000], Loss: 421.6952\n",
      "Epoch [1450/10000000], Loss: 421.4869\n",
      "Epoch [1500/10000000], Loss: 421.3023\n",
      "Epoch [1550/10000000], Loss: 421.1381\n",
      "Epoch [1600/10000000], Loss: 420.9918\n",
      "Epoch [1650/10000000], Loss: 420.8609\n",
      "Epoch [1700/10000000], Loss: 420.7438\n",
      "Epoch [1750/10000000], Loss: 420.6388\n",
      "Epoch [1800/10000000], Loss: 420.5445\n",
      "Epoch [1850/10000000], Loss: 420.4597\n",
      "Epoch [1900/10000000], Loss: 420.3834\n",
      "Epoch [1950/10000000], Loss: 420.3147\n",
      "Epoch [2000/10000000], Loss: 420.2527\n",
      "Epoch [2050/10000000], Loss: 420.1969\n",
      "Epoch [2100/10000000], Loss: 420.1465\n",
      "Epoch [2150/10000000], Loss: 420.1011\n",
      "Epoch [2200/10000000], Loss: 420.0600\n",
      "Epoch [2250/10000000], Loss: 420.0230\n",
      "Epoch [2300/10000000], Loss: 419.9895\n",
      "Epoch [2350/10000000], Loss: 419.9592\n",
      "Epoch [2400/10000000], Loss: 419.9319\n",
      "Epoch [2450/10000000], Loss: 419.9072\n",
      "Epoch [2500/10000000], Loss: 419.8849\n",
      "Epoch [2550/10000000], Loss: 419.8647\n",
      "Epoch [2600/10000000], Loss: 419.8465\n",
      "Epoch [2650/10000000], Loss: 419.8300\n",
      "Epoch [2700/10000000], Loss: 419.8151\n",
      "Epoch [2750/10000000], Loss: 419.8016\n",
      "Epoch [2800/10000000], Loss: 419.7895\n",
      "Epoch [2850/10000000], Loss: 419.7784\n",
      "Epoch [2900/10000000], Loss: 419.7685\n",
      "Epoch [2950/10000000], Loss: 419.7595\n",
      "Epoch [3000/10000000], Loss: 419.7513\n",
      "Epoch [3050/10000000], Loss: 419.7440\n",
      "Epoch [3100/10000000], Loss: 419.7373\n",
      "Epoch [3150/10000000], Loss: 419.7313\n",
      "Epoch [3200/10000000], Loss: 419.7258\n",
      "Epoch [3250/10000000], Loss: 419.7209\n",
      "Epoch [3300/10000000], Loss: 419.7164\n",
      "Epoch [3350/10000000], Loss: 419.7124\n",
      "Epoch [3400/10000000], Loss: 419.7088\n",
      "Epoch [3450/10000000], Loss: 419.7055\n",
      "Epoch [3500/10000000], Loss: 419.7025\n",
      "Epoch [3550/10000000], Loss: 419.6998\n",
      "Epoch [3600/10000000], Loss: 419.6974\n",
      "Epoch [3650/10000000], Loss: 419.6952\n",
      "Epoch [3700/10000000], Loss: 419.6932\n",
      "Epoch [3750/10000000], Loss: 419.6914\n",
      "Epoch [3800/10000000], Loss: 419.6897\n",
      "Epoch [3850/10000000], Loss: 419.6883\n",
      "Epoch [3900/10000000], Loss: 419.6869\n",
      "Epoch [3950/10000000], Loss: 419.6857\n",
      "Epoch [4000/10000000], Loss: 419.6846\n",
      "Epoch [4050/10000000], Loss: 419.6837\n",
      "Epoch [4100/10000000], Loss: 419.6828\n",
      "Epoch [4150/10000000], Loss: 419.6820\n",
      "Epoch [4200/10000000], Loss: 419.6812\n",
      "Epoch [4250/10000000], Loss: 419.6806\n",
      "Epoch [4300/10000000], Loss: 419.6800\n",
      "Epoch [4350/10000000], Loss: 419.6794\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# TRAINING LOOP FOR A MODEL\n",
    "# -----------------------------\n",
    "\n",
    "# Set a small tolerance value — if the loss stops changing by more than this, we’ll stop training early\n",
    "tol = 1e-5  \n",
    "\n",
    "# Set the maximum number of epochs (full passes through the dataset)\n",
    "# 1e7 = 10 million (a very large number, used here as an upper limit)\n",
    "epochs = int(1e7)  \n",
    "\n",
    "# Store the previous loss value; start with infinity so any real loss will be smaller\n",
    "prev_loss = float('inf')  \n",
    "\n",
    "# Loop through each epoch (training iteration)\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # ---- Forward pass ----\n",
    "    # Feed the training data (X_train) to the model to get predictions (outputs)\n",
    "    outputs = model(X_train)\n",
    "\n",
    "    # Calculate how far off the predictions are from the true labels (y_train)\n",
    "    # The 'criterion' defines the loss function (e.g., MSE, cross-entropy, etc.)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # ---- Backward pass & optimization ----\n",
    "    # Clear (reset) any gradients from the previous step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute new gradients for each model parameter based on the current loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model parameters (weights and biases) using the optimizer\n",
    "    optimizer.step()\n",
    "    \n",
    "    # ---- Check progress ----\n",
    "    # Convert the loss (a PyTorch tensor) to a plain Python number\n",
    "    curr_loss = loss.item()\n",
    "    \n",
    "    # Print the current epoch number and loss every 50 epochs to track progress\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {curr_loss:.4f}')\n",
    "    \n",
    "    # ---- Early stopping condition ----\n",
    "    # If the loss hasn’t changed much compared to the last epoch, stop training\n",
    "    if abs(curr_loss - prev_loss) <= tol:\n",
    "        break\n",
    "    \n",
    "    # Save current loss as previous loss for the next iteration\n",
    "    prev_loss = curr_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68538be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./model/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dffee925",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.nn.modules.linear.Linear was not an allowed global by default. Please use `torch.serialization.add_safe_globals([torch.nn.modules.linear.Linear])` or the `torch.serialization.safe_globals([torch.nn.modules.linear.Linear])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./model/model.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ml-practice/venv312/lib/python3.12/site-packages/torch/serialization.py:1529\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1521\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1522\u001b[39m                     opened_zipfile,\n\u001b[32m   1523\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1526\u001b[39m                     **pickle_load_args,\n\u001b[32m   1527\u001b[39m                 )\n\u001b[32m   1528\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1531\u001b[39m             opened_zipfile,\n\u001b[32m   1532\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1535\u001b[39m             **pickle_load_args,\n\u001b[32m   1536\u001b[39m         )\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.nn.modules.linear.Linear was not an allowed global by default. Please use `torch.serialization.add_safe_globals([torch.nn.modules.linear.Linear])` or the `torch.serialization.safe_globals([torch.nn.modules.linear.Linear])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "model = torch.load(\"./model/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d28500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 410.1239\n",
      "Root Mean Squared Error (RMSE): 20.2515\n",
      "Mean Absolute Error (MAE): 16.3524\n",
      "R2 Score: 0.3138\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(X_test)\n",
    "\n",
    "y_test_flat, y_pred_flat = y_test.numpy().flatten(), y_pred.detach().numpy().flatten()\n",
    "\n",
    "mse = mean_squared_error(y_test_flat, y_pred_flat)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_flat, y_pred_flat)\n",
    "r2 = r2_score(y_test_flat, y_pred_flat)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee424fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
